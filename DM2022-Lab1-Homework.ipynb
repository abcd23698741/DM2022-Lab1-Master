{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 陳薇如\n",
    "\n",
    "Student ID: 110003816\n",
    "\n",
    "GitHub ID: abcd23698741\n",
    "https://github.com/abcd23698741/DM2022-Lab1-Master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2022-Lab1-Master](https://github.com/keziatamus/DM2022-Lab1-Master). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2022-Lab1-Master](https://github.com/keziatamus/DM2022-Lab1-Master) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The dataset contains a `sentence` and `score` label. Read the specificiations of the dataset for details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be habdled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/keziatamus/DM2022-Lab1-Homework/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 20th 11:59 pm, Thursday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data Description\n",
    "> Source: https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#\n",
    "> This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015. \n",
    "It contains sentences labelled with positive or negative sentiment.\n",
    "\n",
    "#### Format\n",
    "> sentence score\n",
    "\n",
    "#### Details\n",
    "> Score is either 1 (for positive) or 0 (for negative) The sentences come from three different websites/fields.\n",
    "> For each website, there exist 500 positive and 500 negative sentences. Those were selected randomly for larger datasets of reviews.We attempted to select sentences that have a clearly positive or negative connotaton, the goal was for no neutral sentences to be selected.\n",
    ">- imdb.com\n",
    ">- amazon.com\n",
    ">- yelp.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Steps\n",
    "1. Donwload the data from repository, and save then under /data foler.\n",
    "2. Examine the files, can find out that the data format: <**Sentence + label**> seperated by tab and \\n.\n",
    "3. With this primary acknowledge, split lines into sentence and label when reading file, make the process more efficently.\n",
    "4. Save the result into pandas dataframe **df** for further application.\n",
    "\n",
    "#### Others:\n",
    "- Trimming the leading and trailing whitespaces of sentance\n",
    "- Transform the data type of label from string into integral\n",
    "\n",
    "#### Output:\n",
    "- A dataframe with size of (3000,3), the len is consistant with data description (3 website, 1000 sentence each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re, collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import helpers.data_mining_helpers as dmh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb_labelled.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb_labelled.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb_labelled.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb_labelled.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "      <td>imdb_labelled.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence label             source\n",
       "0  A very, very, very slow-moving, aimless movie ...     0  imdb_labelled.txt\n",
       "1  Not sure who was more lost - the flat characte...     0  imdb_labelled.txt\n",
       "2  Attempting artiness with black & white and cle...     0  imdb_labelled.txt\n",
       "3         Very little music or anything to speak of.     0  imdb_labelled.txt\n",
       "4  The best scene in the movie was when Gerardo i...     1  imdb_labelled.txt"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read txt dat into pandas dataframe\n",
    "data =[]\n",
    "\n",
    "file_list = ['imdb_labelled.txt','yelp_labelled.txt','amazon_cells_labelled.txt']\n",
    "\n",
    "for file in file_list:\n",
    "    file_path = os.path.join('data',file) \n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        raw = f.readlines()\n",
    "        for i in raw:\n",
    "            line = i.replace('\\n','').split('\\t')\n",
    "            data.append([line[0].strip(),line[1],file])\n",
    "df=pd.DataFrame(data, columns=['sentence','label','source'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of raw data: (3000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of raw data:\",df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data cleaning\n",
    "#### Main Steps\n",
    "1. Remove duplicate data \n",
    "2. Drop missing value \n",
    "\n",
    "#### Output\n",
    "1. The len of dataframe **reduce to 2983** after remove duplicate items\n",
    "2. The len of dataframe remain the same after remove missing value (**no missing value**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate#: 17\n",
      "\n",
      "\n",
      "The shape of data after drop duplicated items: (2983, 3)\n"
     ]
    }
   ],
   "source": [
    "#Examime if duplicate and drop\n",
    "if sum(df.duplicated())==0:\n",
    "    print ('no duplicate')\n",
    "else:\n",
    "    print ('duplicate#:',sum(df.duplicated()))\n",
    "    df=df.drop_duplicates()\n",
    "print('\\n')   \n",
    "print('The shape of data after drop duplicated items:',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    (The amoung of missing records is: , 0)\n",
       "label       (The amoung of missing records is: , 0)\n",
       "source      (The amoung of missing records is: , 0)\n",
       "dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examime if missing value and drop\n",
    "df.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Engineering\n",
    "The remaining values are unique, it's meaningless to build a model like this. Therefore, must create **unigram** as features for futher classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abcd2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [A, very, ,, very, ,, very, slow-moving, ,, ai...\n",
       "1    [Not, sure, who, was, more, lost, -, the, flat...\n",
       "2    [Attempting, artiness, with, black, &, white, ...\n",
       "3    [Very, little, music, or, anything, to, speak,...\n",
       "4    [The, best, scene, in, the, movie, was, when, ...\n",
       "Name: unigrams, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Unigram\n",
    "df['unigrams'] = df['sentence'].apply(lambda x: dmh.tokenize_text(x))\n",
    "df[\"unigrams\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>unigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb_labelled.txt</td>\n",
       "      <td>[A, very, ,, very, ,, very, slow-moving, ,, ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb_labelled.txt</td>\n",
       "      <td>[Not, sure, who, was, more, lost, -, the, flat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb_labelled.txt</td>\n",
       "      <td>[Attempting, artiness, with, black, &amp;, white, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb_labelled.txt</td>\n",
       "      <td>[Very, little, music, or, anything, to, speak,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "      <td>imdb_labelled.txt</td>\n",
       "      <td>[The, best, scene, in, the, movie, was, when, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence label             source  \\\n",
       "0  A very, very, very slow-moving, aimless movie ...     0  imdb_labelled.txt   \n",
       "1  Not sure who was more lost - the flat characte...     0  imdb_labelled.txt   \n",
       "2  Attempting artiness with black & white and cle...     0  imdb_labelled.txt   \n",
       "3         Very little music or anything to speak of.     0  imdb_labelled.txt   \n",
       "4  The best scene in the movie was when Gerardo i...     1  imdb_labelled.txt   \n",
       "\n",
       "                                            unigrams  \n",
       "0  [A, very, ,, very, ,, very, slow-moving, ,, ai...  \n",
       "1  [Not, sure, who, was, more, lost, -, the, flat...  \n",
       "2  [Attempting, artiness, with, black, &, white, ...  \n",
       "3  [Very, little, music, or, anything, to, speak,...  \n",
       "4  [The, best, scene, in, the, movie, was, when, ...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The dataframe after adding unigrams\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sentence: \"A very, very, very slow-moving, aimless movie about a distressed, drifting young man.\"\n",
      "  (0, 4890)\t3\n",
      "  (0, 4133)\t1\n",
      "  (0, 2956)\t1\n",
      "  (0, 166)\t1\n",
      "  (0, 2954)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 1331)\t1\n",
      "  (0, 1401)\t1\n",
      "  (0, 5139)\t1\n",
      "  (0, 2764)\t1\n"
     ]
    }
   ],
   "source": [
    "#Feature subset selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "df_counts = count_vect.fit_transform(df.sentence) #learn the vocabulary and return document-term matrix\n",
    "\n",
    "#Feature name mapping\n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "print('The first sentence: \"%s\"' %df.sentence[0])\n",
    "print(df_counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For examination:\n",
      "\n",
      "The #4890 feature is: \"very\"\n",
      "The #4133 feature is: \"slow\"\n",
      "\n",
      "Match the the word freqency of the original sentence!\n"
     ]
    }
   ],
   "source": [
    "#Examine the result after applying CountVectorizer\n",
    "print('For examination:\\n')\n",
    "print('The #4890 feature is: \"%s\"' %feature_names[4890])\n",
    "print('The #4133 feature is: \"%s\"\\n' %feature_names[4133])\n",
    "print('Match the the word freqency of the original sentence!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2983, 5155)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of new dataframe\n",
    "df_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>15g</th>\n",
       "      <th>15pm</th>\n",
       "      <th>17</th>\n",
       "      <th>...</th>\n",
       "      <th>yucky</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yun</th>\n",
       "      <th>z500a</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombiez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  10  100  11  12  13  15  15g  15pm  17   ...     yucky  yukon  yum  \\\n",
       "0   0   0    0   0   0   0   0    0     0   0   ...         0      0    0   \n",
       "1   0   0    0   0   0   0   0    0     0   0   ...         0      0    0   \n",
       "2   0   0    0   0   0   0   0    0     0   0   ...         0      0    0   \n",
       "3   0   0    0   0   0   0   0    0     0   0   ...         0      0    0   \n",
       "4   0   0    0   0   0   0   0    0     0   0   ...         0      0    0   \n",
       "\n",
       "   yummy  yun  z500a  zero  zillion  zombie  zombiez  \n",
       "0      0    0      0     0        0       0        0  \n",
       "1      0    0      0     0        0       0        0  \n",
       "2      0    0      0     0        0       0        0  \n",
       "3      0    0      0     0        0       0        0  \n",
       "4      0    0      0     0        0       0        0  \n",
       "\n",
       "[5 rows x 5155 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#organize as dataframe type\n",
    "df_counts = pd.DataFrame(df_counts.toarray(), columns=feature_names)\n",
    "df_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
